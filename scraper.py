# scraper.py

import os  # Dosya iÅŸlemleri iÃ§in (klasÃ¶r oluÅŸturma, dosya kaydetme)
import requests  # Web'den veri Ã§ekmek iÃ§in
from bs4 import BeautifulSoup  # HTML iÃ§eriÄŸini parse (inceleme) etmek iÃ§in
from urllib.parse import urljoin  # GÃ¶reli URL'leri tam URL'ye Ã§evirmek iÃ§in kullanÄ±lÄ±r

# ğŸ’¡ Bu fonksiyon, bir URL'den belirli bir veri tipini (gÃ¶rsel, baÅŸlÄ±k, link) kazÄ±mak iÃ§in kullanÄ±lÄ±r
def start_scraping(url, data_type):
    response = requests.get(url)  # Web sayfasÄ±nÄ± indirir
    soup = BeautifulSoup(response.text, "html.parser")  # HTML'yi analiz edilebilir hale getirir

    os.makedirs("data", exist_ok=True)  # ğŸ’¡ Bu klasÃ¶r yoksa oluÅŸturur, varsa hata vermez (sÄ±k kullanÄ±lan gÃ¼venli yÃ¶ntemdir)

    # KullanÄ±cÄ±nÄ±n seÃ§tiÄŸi veri tÃ¼rÃ¼ne gÃ¶re iÅŸlem yapÄ±lÄ±r
    if data_type == "images":
        count = 0

        # TÃ¼m <img> taglerini bulur
        images = soup.find_all("img")

        # AyrÄ±ca CSS iÃ§inde background-image geÃ§en tagleri de bulur
        bg_tags = soup.find_all(style=lambda s: s and 'background-image' in s)

        # --- 1. GerÃ§ek <img> tag'lerinden gelen gÃ¶rseller
        for img in images:
            # BazÄ± sitelerde 'src' yerine 'data-src' kullanÄ±lÄ±r (lazy loading)
            src = img.get("src") or img.get("data-src")
            if not src:
                continue
            full_url = urljoin(url, src)  # ğŸ’¡ GÃ¶reli URL'yi tam URL'ye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
            count += download_image(full_url, count)

        # --- 2. CSS iÃ§indeki background-image: url(...) ÅŸeklinde gelen gÃ¶rseller
        for tag in bg_tags:
            style = tag.get("style")
            start = style.find("url(")
            end = style.find(")", start)
            if start != -1 and end != -1:
                bg_url = style[start + 4:end].strip('\'"')  # Parantez ve tÄ±rnaklardan arÄ±ndÄ±r
                full_url = urljoin(url, bg_url)
                count += download_image(full_url, count)

        return count

    elif data_type == "titles":
        # <h1>, <h2>, <h3> gibi baÅŸlÄ±k tag'lerini seÃ§er
        titles = [tag.text.strip() for tag in soup.find_all(["h1", "h2", "h3"])]
        with open("data/titles.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(titles))  # TÃ¼m baÅŸlÄ±klarÄ± tek dosyaya yaz
        return len(titles)

    elif data_type == "links":
        # TÃ¼m <a href="..."> linklerini bul
        links = [a.get("href") for a in soup.find_all("a", href=True)]
        with open("data/links.txt", "w", encoding="utf-8") as f:
            f.write("\n".join(links))  # Linkleri dosyaya yaz
        return len(links)

    else:
        raise ValueError("Bilinmeyen veri tÃ¼rÃ¼")  # ğŸ’¡ GÃ¼venlik iÃ§in: TanÄ±msÄ±z veri tÃ¼rÃ¼ verilirse hata fÄ±rlatÄ±r

# ğŸ’¡ Bu fonksiyon tek bir gÃ¶rseli indirip 'data/' klasÃ¶rÃ¼ne kaydeder
def download_image(url, count):
    try:
        img_data = requests.get(url).content
        filename = os.path.join("data", f"img_{count}.jpg")
        with open(filename, "wb") as f:
            f.write(img_data)
        return 1  # BaÅŸarÄ±lÄ± indirme
    except:
        return 0  # Hata olursa pas geÃ§
